{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush, heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eda2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acbdcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe = {}\n",
    "with open(\"glove.6B/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as vector_file:\n",
    "    for line in vector_file:\n",
    "        line_content = line.split()\n",
    "        word = line_content[0]\n",
    "        # There's probably a better way to read strings into a FloatTensor\n",
    "        word_vec = torch.from_numpy(np.asarray(line_content[1:], \"float32\"))\n",
    "        GloVe[word] = word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d599fe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548,\n",
      "         0.0635, -0.0942,  0.1579, -0.8166,  0.1417,  0.2194,  0.5850, -0.5216,\n",
      "         0.2278, -0.1664, -0.6823,  0.3587,  0.4257,  0.1902,  0.9196,  0.5756,\n",
      "         0.4618,  0.4236, -0.0954, -0.4275, -0.1657, -0.0568, -0.2959,  0.2604,\n",
      "        -0.2661, -0.0704, -0.2766,  0.1582,  0.6982,  0.4308,  0.2795, -0.4544,\n",
      "        -0.3380, -0.5818,  0.2236, -0.5778, -0.2686, -0.2042,  0.5639, -0.5852,\n",
      "        -0.1436, -0.6422,  0.0055, -0.3525,  0.1616,  1.1796, -0.4767, -2.7553,\n",
      "        -0.1321, -0.0477,  1.0655,  1.1034, -0.2208,  0.1867,  0.1318,  0.1512,\n",
      "         0.7131, -0.3521,  0.9135,  0.6178,  0.7099,  0.2395, -0.1457, -0.3786,\n",
      "        -0.0460, -0.4737,  0.2385,  0.2054, -0.1900,  0.3251, -1.1112, -0.3634,\n",
      "         0.9868, -0.0848, -0.5401,  0.1173, -1.0194, -0.2442,  0.1277,  0.0139,\n",
      "         0.0804, -0.3541,  0.3495, -0.7226,  0.3755,  0.4441, -0.9906,  0.6121,\n",
      "        -0.3511, -0.8316,  0.4529,  0.0826])\n"
     ]
    }
   ],
   "source": [
    "# Play with GloVe embeddings\n",
    "print(GloVe[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2897617",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bank = []\n",
    "#https://github.com/mjhea0/twitter-sentiment-analysis/blob/master/wordbanks/positive-words.txt\n",
    "#with open(\"pos_sentiment.txt\", \"r\") as pos_sent_txt:\n",
    "#    lines = pos_sent_txt.read().splitlines() \n",
    "#    word_bank = lines\n",
    "word_bank = ['fearful','terrified','suspicious','anxious','alarmed','panic','nervous','scared','worried','frightened','timid','shaky','restless','doubtful','threatened','cowardly','quaking','wary','dejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615f5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Word Bank\n",
    "#word_bank = [\"academy\", \"advance\", \"aircraft\", \"ally\", \"ammo\", \"ammunition\", \"armor\", \"arms\", \"army\", \"arrow\", \"arsenal\", \"artillery\", \"attack\", \"attention\", \"ballistic\", \"barracks\", \"base\", \"battalion\", \"battery\", \"battle\", \"battlefield\", \"bomb\", \"bombard\", \"bombardment\", \"brig\", \"brigade\", \"bullet\", \"camouflage\", \"camp\", \"cannon\", \"captain\", \"capture\", \"carrier\", \"casualty\", \"catapult\", \"cavalry\", \"colonel\", \"combat\", \"command\", \"commander\", \"commission\", \"company\", \"conflict\", \"conquest\", \"convoy\", \"corps\", \"covert\", \"crew\", \"decode\", \"defeat\", \"defend\", \"defense\", \"destroyer\", \"division\", \"draft\", \"encode\", \"enemy\", \"engage\", \"enlist\", \"evacuate\", \"explosive\", \"fight\", \"fire\", \"fleet\", \"force\", \"formation\", \"fort\", \"front\", \"garrison\", \"general\", \"grenade\", \"grunt\", \"guerrilla\", \"gun\", \"headquarters\", \"helmet\", \"honor\", \"hospital\", \"infantry\", \"injury\", \"intelligence\", \"invade\", \"invasion\", \"jet\", \"kill\", \"leave\", \"lieutenant\", \"major\", \"maneuver\", \"marines\", \"MIA\", \"mid\", \"military\", \"mine\", \"missile\", \"mortar\", \"navy\", \"neutral\", \"offense\", \"officer\", \"ordinance\", \"parachute\", \"peace\", \"plane\", \"platoon\", \"private\", \"radar\", \"rank\", \"recruit\", \"regiment\", \"rescue\", \"reserves\", \"retreat\", \"ribbon\", \"sabotage\", \"sailor\", \"salute\", \"section\", \"sergeant\", \"service\", \"shell\", \"shoot\", \"shot\", \"siege\", \"sniper\", \"soldier\", \"spear\", \"specialist\", \"squad\", \"squadron\", \"staff\", \"submarine\", \"surrender\", \"tactical\", \"tactics\", \"tank\", \"torpedo\", \"troops\", \"truce\", \"uniform\", \"unit\", \"veteran\", \"volley\", \"war\", \"warfare\", \"warrior\", \"weapon\", \"win\", \"wound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21576b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Word Embeddings Matrix\n",
    "wb_embeddings = torch.zeros((len(word_bank), 100))\n",
    "#print(word_bank)\n",
    "for i, word in enumerate(word_bank):\n",
    "    if word.lower() in GloVe:\n",
    "        wb_embeddings[i] = GloVe[word.lower()]\n",
    "    \n",
    "wb_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caf0463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_score(embedding):\n",
    "    distances = wb_embeddings - embedding\n",
    "    return float(torch.linalg.norm(distances, dim=0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93036fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_idx(sorted_vals):\n",
    "    softmax_scores = sorted_vals.softmax(dim=-1).detach().numpy()\n",
    "    \n",
    "    ret = np.random.choice(softmax_scores, p=softmax_scores)\n",
    "    #print(ret)\n",
    "    return np.where(softmax_scores==ret)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9de54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p(sorted_vals, indices):\n",
    "    trunc_sorted_vals = []\n",
    "    sum_so_far = 0\n",
    "    # reversed?\n",
    "    for val in reversed(sorted_vals):\n",
    "        sum_so_far += val\n",
    "        trunc_sorted_vals.append(val)\n",
    "        if sum_so_far > top_p_val:\n",
    "            break\n",
    "    sorted_vals = torch.FloatTensor(trunc_sorted_vals)\n",
    "    indices = indices[-len(sorted_vals):]\n",
    "    return sorted_vals, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sorted_vals, indices, top_embeddings):\n",
    "    for word_idx in range(len(indices)):\n",
    "    word = tokenizer.decode(indices[word_idx])\n",
    "    if word.strip().lower() not in GloVe.keys():\n",
    "        sorted_vals[word_idx] = 0  # disregard this token\n",
    "        top_embeddings.append(GloVe['failure']) # TOFIX\n",
    "    else:\n",
    "        if word[1:].isalpha() or word.isalpha():\n",
    "            top_embeddings.append(GloVe[word.strip().lower()])\n",
    "        else:\n",
    "            top_embeddings.append(GloVe[word.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66109d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0000001\n",
    "exponent = 1\n",
    "def rerank(sorted_vals, dist_score, hyper_weight):\n",
    "    # pre_rerank = sorted_vals.detach().clone()\n",
    "    # re-rank the weightings, factor in dist_score\n",
    "    \n",
    "    sorted_vals += (((1 / (dist_score + eps)) ** exponent) * hyper_weight)\n",
    "    \n",
    "    sorted_vals = sorted_vals.softmax(dim=-1)\n",
    "    sort_indices = torch.argsort(sorted_vals)\n",
    "    sorted_vals = sorted_vals[sort_indices]\n",
    "    final_ranked_indices = indices[sort_indices]\n",
    "    #final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "    \n",
    "    return final_ranked_indices, sorted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ccce1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 Output:\n",
      "[\"When I think about it, I feel like I'm in the middle of something. I feel like I'm in the middle of something. I feel like I'm in the middle of something. I\"]\n",
      "Custom Output: \n",
      "When I think about it, I feel really excited about how things could work out for him,\" Goodell said. \"The Dolphins got good offense from quarterback to quarterback and\n"
     ]
    }
   ],
   "source": [
    "# new implementation, using GloVe vectors\n",
    "\n",
    "# TODO: Custom Beam Search -- Keep n possibilities (beams) at each time\n",
    "# then, accumulate a probability associated with each (normalize by length of generation)\n",
    "\n",
    "# indices = token_ids\n",
    "\n",
    "# March 1st: Sampling, performance, normalization\n",
    "prompt = \"When I think about it, I feel\"\n",
    "\n",
    "print(\"Base GPT-2 Output:\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "gpt2_output = tokenizer.batch_decode(model.generate(**inputs, num_beams=4, do_sample=False, max_new_tokens=32, pad_token_id=50256))\n",
    "print(gpt2_output)\n",
    "\n",
    "top_k_val = 10\n",
    "top_p_val = 0.7\n",
    "NUM_TOK_TO_GEN = 25\n",
    "NUM_BEAMS = 2\n",
    "HYPER_WEIGHT = 5\n",
    "\n",
    "# generate one word given a prompt_beam\n",
    "def generate_one(prompt_beam, idx):\n",
    "    prompt = prompt_beam[0]\n",
    "    score = prompt_beam[1]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    #loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "    sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "    \n",
    "    # Calculate Top-P\n",
    "    if top_p_val > 0:\n",
    "        sorted_vals, indices = top_p(sorted_vals[:], indices[:])\n",
    "    else:\n",
    "        # else, we just do top-k\n",
    "        sorted_vals = sorted_vals[-top_k_val:]\n",
    "        indices = indices[-top_k_val:]\n",
    "\n",
    "    #print([tokenizer.decode(word) for word in indices])\n",
    "\n",
    "    top_embeddings = [] \n",
    "    generate_embeddings(sorted_vals, indices, top_embeddings)\n",
    "\n",
    "\n",
    "    #top_embeddings = [GloVe[tokenizer.decode(word).strip().lower()] for word in indices]\n",
    "\n",
    "    # calculate distance to cluster\n",
    "    dist_score = [distance_score(embed) for embed in top_embeddings]\n",
    "\n",
    "    # sorted_vals are softmaxed logits\n",
    "    final_ranked_indices, sorted_vals = rerank(sorted_vals, indices, dist_score, HYPER_WEIGHT)\n",
    "\n",
    "    # replace -1 with -idx for true beam search\n",
    "    # add variability instead for true decoding (TODO)\n",
    "    # TODO normalization\n",
    "    \n",
    "    # must sample index if we use top_p\n",
    "    if top_p_val > 0:\n",
    "        idx = sample_idx(sorted_vals[:])\n",
    "    \n",
    "    best_word = tokenizer.decode(final_ranked_indices[-idx])\n",
    "    prompt += best_word\n",
    "\n",
    "    # add normalization by length\n",
    "\n",
    "\n",
    "    #return [prompt, score + s_vals[-idx].detach().numpy()]\n",
    "    \n",
    "    #(1/len(prompt)+1) *\n",
    "    # adjusted to ensure that we keep generating more words.\n",
    "    # otherwise, we stop almost immediately since the probability of the\n",
    "    # second word is 20%, the probability of the first guessed word was ~80%\n",
    "    return [prompt, len(prompt) + sorted_vals[-idx].detach().numpy()] # subject to change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(num_beams, tokens_to_generate):\n",
    "    beams = [[prompt, 0]]\n",
    "\n",
    "    #for token_num in range(NUM_TOK_TO_GEN):\n",
    "\n",
    "    for token_num in range(tokens_to_generate):\n",
    "        num_to_investigate = len(beams)\n",
    "        for beam_idx in range(num_to_investigate):\n",
    "            prompt_beam = beams[beam_idx]\n",
    "            for position in range(num_beams):\n",
    "                ret = generate_one(prompt_beam, position)\n",
    "                beams.append(ret)\n",
    "        # or normalize scores by length here\n",
    "        beams = sorted(beams, key=lambda x: -x[1])\n",
    "        beams = beams[:num_beams]\n",
    "    return beams\n",
    "\n",
    "result = beam_search(NUM_BEAMS, NUM_TOK_TO_GEN)\n",
    "print('Custom Output: ')\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f50010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "def get_perspective_score(text):\n",
    "    API_KEY = \"AIzaSyBhoNi1Om9gxeVIGcMdlLfTCeXYQgb8EU8\"\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "    )\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': f'{text}' },\n",
    "      'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response['attributeScores']['TOXICITY']['summaryScore']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a2c2049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 toxicity score: 0.10089093\n",
      "Custom output toxicity score: 0.36095104\n"
     ]
    }
   ],
   "source": [
    "custom_res = result[0][0]\n",
    "base_score = get_perspective_score(gpt2_output)\n",
    "custom_score = get_perspective_score(custom_res)\n",
    "print(f'GPT2 toxicity score: {base_score}')\n",
    "print(f'Custom output toxicity score: {custom_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
