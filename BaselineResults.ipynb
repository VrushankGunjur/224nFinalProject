{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "887c0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6260c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/Users/vrushankgunjur/documents/224n/final_proj'\n",
    "GPT2_PATH = f'{BASE_PATH}/GPT2BaseResults.txt'\n",
    "CUSTOMP_PATH = f'{BASE_PATH}/top_p.txt'\n",
    "CUSTOMK_PATH = f'{BASE_PATH}/top_k.txt'\n",
    "CUSTOMDOT_PATH = f'{BASE_PATH}/dotresults.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fcd43202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df\n",
    "\n",
    "def get_dataframe(path):\n",
    "    res = []\n",
    "    with open(path, 'r', encoding=\"utf-8\") as results:\n",
    "        lines = results.readlines()\n",
    "        for line in lines[1:]:\n",
    "            parts = line.split(',')\n",
    "            #print(parts)\n",
    "\n",
    "            full_prompt = \"\"\n",
    "            #print(parts)\n",
    "            prompt_pieces = parts[:-4]\n",
    "            for piece in prompt_pieces:\n",
    "                full_prompt += piece\n",
    "            res.append({'Prompt': full_prompt, 'Max_Toxicity': float(parts[-4]), 'Num_Toxic': int(parts[-3]), 'Num_Analyzed': int(parts[-2]), 'Mean_Toxicity': float((parts[-1]))})\n",
    "    #print(res)\n",
    "    df = pd.DataFrame(res)\n",
    "    df = df[:200]\n",
    "    #df = df[:150]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2e1b0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def failure_percent(df):\n",
    "    return 1 - (df['Num_Analyzed'].sum() / (len(df)*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "78a4c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_df(df):\n",
    "    df['True_Mean'] = df['Mean_Toxicity'] * df['Num_Analyzed']\n",
    "    df['True_Mean'] /= df['Num_Analyzed']\n",
    "    mean_toxicity = df['True_Mean'].mean()\n",
    "    \n",
    "    print(f'N: {len(df)}')\n",
    "    print(f'Mean Toxicity: {mean_toxicity}')\n",
    "\n",
    "    mean_max_toxicity = df['Max_Toxicity'].mean()\n",
    "    std_max_toxicity = df['Max_Toxicity'].std()\n",
    "    print(f'Average Maximum Toxicity: {mean_max_toxicity}')\n",
    "    print(f'Average Maximum STDEV: {std_max_toxicity}')\n",
    "\n",
    "    # note that max toxicity is scuffed since it doesn't account for less than 5 sampled (jank responses)\n",
    "    # this is particularly relevant to custom output\n",
    "\n",
    "\n",
    "    #mean_num_toxic = df['Num_Toxic'].mean()\n",
    "    #print(f'DOESNT WORK Mean # Toxic: {mean_num_toxic}')\n",
    "\n",
    "    #print(len(df[df['Num_Toxic'] == 5]))\n",
    "\n",
    "    percent_toxic1 = len(df[df['Num_Toxic'] != 0]) / (len(df))\n",
    "\n",
    "    print(f'Percent Toxic (Toxic > 0 / Num Analyzed, their metric): {percent_toxic1}')\n",
    "    #print(df['Num_Toxic'].sum())\n",
    "    percent_toxic2 = df['Num_Toxic'].sum() / (df['Num_Analyzed'].sum())\n",
    "    #print(df['Num_Toxic'])\n",
    "    print(f'Percent Toxic (# Toxic tot / Num Analyzed, our metric): {percent_toxic2}')\n",
    "    \n",
    "    fp = failure_percent(df)\n",
    "    print(f'Failure Percent: {fp}')\n",
    "    \n",
    "    return {'Mean Toxicity':mean_toxicity,'AvgMaxToxicity': mean_max_toxicity, \n",
    "            'AvgMaxToxicityStd': std_max_toxicity, 'PercentToxic1': percent_toxic1, 'PercentToxic2': percent_toxic2, 'FailurePercent': fp}\n",
    "    #print(f'Failure Percent GPT2: {failure_percent(gpt2_df)}')\n",
    "    #more_toxic = df[df['Num_Toxic'] > 1]\n",
    "    #print(more_toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d6915e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2: \n",
      "N: 200\n",
      "Mean Toxicity: 0.572677070274\n",
      "Average Maximum Toxicity: 0.71276679473\n",
      "Average Maximum STDEV: 0.1979964489595504\n",
      "Percent Toxic (Toxic > 0 / Num Analyzed, their metric): 0.835\n",
      "Percent Toxic (# Toxic tot / Num Analyzed, our metric): 0.639\n",
      "Failure Percent: 0.0\n",
      "----------\n",
      "CUSTOM TOP-P (DIST): \n",
      "N: 200\n",
      "Mean Toxicity: 0.539614384384\n",
      "Average Maximum Toxicity: 0.66646441375\n",
      "Average Maximum STDEV: 0.19719628144592458\n",
      "Percent Toxic (Toxic > 0 / Num Analyzed, their metric): 0.755\n",
      "Percent Toxic (# Toxic tot / Num Analyzed, our metric): 0.5877016129032258\n",
      "Failure Percent: 0.008000000000000007\n",
      "----------\n",
      "CUSTOM TOP-K (DIST): \n",
      "----------\n",
      "CUSTOM DOT (TOP-P): \n",
      "N: 200\n",
      "Mean Toxicity: 0.5474182479009166\n",
      "Average Maximum Toxicity: 0.6685957957700001\n",
      "Average Maximum STDEV: 0.2094762551665043\n",
      "Percent Toxic (Toxic > 0 / Num Analyzed, their metric): 0.765\n",
      "Percent Toxic (# Toxic tot / Num Analyzed, our metric): 0.5945674044265593\n",
      "Failure Percent: 0.006000000000000005\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Toxicity</th>\n",
       "      <th>AvgMaxToxicity</th>\n",
       "      <th>AvgMaxToxicityStd</th>\n",
       "      <th>PercentToxic1</th>\n",
       "      <th>PercentToxic2</th>\n",
       "      <th>FailurePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572677</td>\n",
       "      <td>0.712767</td>\n",
       "      <td>0.197996</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.639000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.539614</td>\n",
       "      <td>0.666464</td>\n",
       "      <td>0.197196</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.587702</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.547418</td>\n",
       "      <td>0.668596</td>\n",
       "      <td>0.209476</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.594567</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mean Toxicity  AvgMaxToxicity  AvgMaxToxicityStd  PercentToxic1  \\\n",
       "0       0.572677        0.712767           0.197996          0.835   \n",
       "1       0.539614        0.666464           0.197196          0.755   \n",
       "2       0.547418        0.668596           0.209476          0.765   \n",
       "\n",
       "   PercentToxic2  FailurePercent  \n",
       "0       0.639000           0.000  \n",
       "1       0.587702           0.008  \n",
       "2       0.594567           0.006  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customp_df = get_dataframe(CUSTOMP_PATH)\n",
    "customk_df = get_dataframe(CUSTOMK_PATH)\n",
    "customdot_df = get_dataframe(CUSTOMDOT_PATH)\n",
    "gpt2_df = get_dataframe(GPT2_PATH)\n",
    "\n",
    "data_list = []\n",
    "print('GPT2: ')\n",
    "data_list.append(analyze_df(gpt2_df))\n",
    "print('----------')\n",
    "print('CUSTOM TOP-P (DIST): ')\n",
    "data_list.append(analyze_df(customp_df))\n",
    "print('----------')\n",
    "print('CUSTOM TOP-K (DIST): ')\n",
    "#data_list.append(analyze_df(customk_df))\n",
    "print('----------')\n",
    "print('CUSTOM DOT (TOP-P): ')\n",
    "data_list.append(analyze_df(customdot_df))\n",
    "\n",
    "\n",
    "heatmap_df = pd.DataFrame(data_list)\n",
    "heatmap_df\n",
    "#sns.heatmap(heatmap_df['Mean Toxicity'])\n",
    "#print(custom_df[custom_df['Num_Analyzed'] <= 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674f8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
