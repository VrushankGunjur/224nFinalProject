{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11a450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93eda2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -35.2360,  -35.3263,  -38.9751,  ...,  -44.4643,  -43.9972,\n",
      "           -36.4578],\n",
      "         [-112.6171, -114.5833, -116.5725,  ..., -119.0127, -118.8060,\n",
      "          -111.6918],\n",
      "         [ -88.7435,  -89.8644,  -93.1977,  ...,  -92.3839,  -96.1783,\n",
      "           -92.1273],\n",
      "         [ -85.1646,  -88.3379,  -92.8703,  ...,  -99.8017,  -94.7656,\n",
      "           -90.9330],\n",
      "         [-116.7279, -119.3948, -121.7259,  ..., -129.1003, -124.6101,\n",
      "          -121.6091],\n",
      "         [ -77.4425,  -80.4463,  -88.0497,  ...,  -96.2564,  -93.6346,\n",
      "           -84.0666]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e17ba509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob     | word\n",
      "---------------\n",
      "0.076539 |  have   \n",
      "0.035736 |  use    \n",
      "0.035068 |  get    \n",
      "0.030946 |  take   \n",
      "0.029044 |  find   \n"
     ]
    }
   ],
   "source": [
    "prompt = \"In the garden, I usually\"\n",
    "top_k_val = 5\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "\n",
    "print('prob     | word')\n",
    "print('---------------')\n",
    "for word_idx in range(1, top_k_val+1):\n",
    "    print(f'{sorted_vals[-word_idx]:5f} | {tokenizer.decode(indices[-word_idx]):8s}')\n",
    "\n",
    "#next_token = next_token_scores.argmax().unsqueeze(0).unsqueeze(0)\n",
    "#print(tokenizer.decode(next_token[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a556735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
