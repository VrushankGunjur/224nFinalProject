{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush, heapify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93eda2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbdcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe = {}\n",
    "with open(\"glove.6B/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as vector_file:\n",
    "    for line in vector_file:\n",
    "        line_content = line.split()\n",
    "        word = line_content[0]\n",
    "        # There's probably a better way to read strings into a FloatTensor\n",
    "        word_vec = torch.from_numpy(np.asarray(line_content[1:], \"float32\"))\n",
    "        GloVe[word] = word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d599fe59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548,\n",
      "         0.0635, -0.0942,  0.1579, -0.8166,  0.1417,  0.2194,  0.5850, -0.5216,\n",
      "         0.2278, -0.1664, -0.6823,  0.3587,  0.4257,  0.1902,  0.9196,  0.5756,\n",
      "         0.4618,  0.4236, -0.0954, -0.4275, -0.1657, -0.0568, -0.2959,  0.2604,\n",
      "        -0.2661, -0.0704, -0.2766,  0.1582,  0.6982,  0.4308,  0.2795, -0.4544,\n",
      "        -0.3380, -0.5818,  0.2236, -0.5778, -0.2686, -0.2042,  0.5639, -0.5852,\n",
      "        -0.1436, -0.6422,  0.0055, -0.3525,  0.1616,  1.1796, -0.4767, -2.7553,\n",
      "        -0.1321, -0.0477,  1.0655,  1.1034, -0.2208,  0.1867,  0.1318,  0.1512,\n",
      "         0.7131, -0.3521,  0.9135,  0.6178,  0.7099,  0.2395, -0.1457, -0.3786,\n",
      "        -0.0460, -0.4737,  0.2385,  0.2054, -0.1900,  0.3251, -1.1112, -0.3634,\n",
      "         0.9868, -0.0848, -0.5401,  0.1173, -1.0194, -0.2442,  0.1277,  0.0139,\n",
      "         0.0804, -0.3541,  0.3495, -0.7226,  0.3755,  0.4441, -0.9906,  0.6121,\n",
      "        -0.3511, -0.8316,  0.4529,  0.0826])\n"
     ]
    }
   ],
   "source": [
    "# Play with GloVe embeddings\n",
    "print(GloVe[','])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ccce1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 Output:\n",
      "['In the garden, I usually keep an open gate in for the door to use']\n",
      "In the garden, I usually have a couple of pots and a couple of pans. I usually have a couple of pots and a\n"
     ]
    }
   ],
   "source": [
    "# new implementation, using GloVe vectors\n",
    "\n",
    "# TODO: Custom Beam Search -- Keep n possibilities (beams) at each time\n",
    "# then, accumulate a probability associated with each (normalize by length of generation)\n",
    "prompt = \"In the garden, I usually\"\n",
    "\n",
    "print(\"Base GPT-2 Output:\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(tokenizer.batch_decode(model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=20)))\n",
    "\n",
    "\n",
    "hate_vector = GloVe['hate']\n",
    "\n",
    "top_k_val = 5\n",
    "NUM_TOK_TO_GEN = 40\n",
    "\n",
    "# take positional argument for which index to pick\n",
    "def generate_one(prompt_beam, idx):\n",
    "    prompt = prompt_beam[0]\n",
    "    score = prompt_beam[1]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    #print(outputs)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "    sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "    sorted_vals = sorted_vals[-top_k_val:]\n",
    "    indices = indices[-top_k_val:]\n",
    "\n",
    "    #print([tokenizer.decode(word) for word in indices])\n",
    "    # one of the generated words is blank (ACTUALLY \\n)\n",
    "    top_embeddings = []\n",
    "    for word_idx in range(len(indices)):\n",
    "        word = tokenizer.decode(indices[word_idx])\n",
    "        if word.strip().lower() not in GloVe.keys():\n",
    "            sorted_vals[word_idx] = 0  # disregard this token\n",
    "            #print(f'word: {word}')\n",
    "            top_embeddings.append(GloVe['failure'])\n",
    "        else:\n",
    "            if word[1:].isalpha():\n",
    "                top_embeddings.append(GloVe[word.strip().lower()])\n",
    "            else:\n",
    "                # TODO: getting here with 'A'\n",
    "                top_embeddings.append(GloVe[word.strip()])\n",
    "\n",
    "\n",
    "    #top_embeddings = [GloVe[tokenizer.decode(word).strip().lower()] for word in indices]\n",
    "    #print(top_embeddings)\n",
    "\n",
    "    dist_score = [torch.linalg.norm(embed-hate_vector) for embed in top_embeddings]\n",
    "    #print(dist_score)\n",
    "\n",
    "    hyper_weight = .5\n",
    "\n",
    "    pre_rerank = sorted_vals.detach().clone()\n",
    "    # re-rank the weightings, factor in dist_score\n",
    "    for i in range(len(sorted_vals)):\n",
    "        if dist_score[i] == 0:\n",
    "            # avoid division by 0 (exact match)\n",
    "            sorted_vals[i] = 0\n",
    "        else:\n",
    "            sorted_vals[i] += (1/dist_score[i])*hyper_weight\n",
    "\n",
    "    sort_indices = torch.argsort(sorted_vals)\n",
    "    final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "\n",
    "    # replace -1 with -idx for true beam search\n",
    "    # add variability instead for true decoding (TODO)\n",
    "    # TODO normalization\n",
    "    \n",
    "    best_word = tokenizer.decode(final_ranked_indices[-idx])\n",
    "    prompt += best_word\n",
    "    s_vals = sorted_vals[sort_indices]\n",
    "    # add normalization by length\n",
    "    return [prompt, score + s_vals[-idx].detach().numpy()]\n",
    "\n",
    "\n",
    "def beam_search(num_beams, tokens_to_generate):\n",
    "    beams = [[prompt, 0]]\n",
    "\n",
    "    #for token_num in range(NUM_TOK_TO_GEN):\n",
    "\n",
    "    for token_num in range(tokens_to_generate):\n",
    "        num_to_investigate = len(beams)\n",
    "        for beam_idx in range(num_to_investigate):\n",
    "            prompt_beam = beams[beam_idx]\n",
    "            for position in range(4):\n",
    "                ret = generate_one(prompt_beam, position)\n",
    "                beams.append(ret)\n",
    "        # or normalize scores by length here\n",
    "        beams = sorted(beams, key=lambda x: -x[1])\n",
    "        #print(beams)\n",
    "        beams = beams[:num_beams]\n",
    "        #print(beams)\n",
    "    return beams\n",
    "\n",
    "result = beam_search(3, 20)\n",
    "print(result[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(f'Generation {token_num}')\n",
    "        print(\"Original: \")\n",
    "        for idx in range(1, top_k_val+1):\n",
    "            #print(tokenizer.decode(indices[-idx]))\n",
    "            print(f'{pre_rerank[-idx]:5f} | {tokenizer.decode(indices[-idx]):8s}')\n",
    "\n",
    "        print(\"\\nAfter Re-rank:\")\n",
    "        s_vals = sorted_vals[sort_indices]\n",
    "        for idx in range(1, len(final_ranked_indices)+1):\n",
    "            #print(s_vals)\n",
    "            #print(idx)\n",
    "            print(f'{s_vals[-idx]:5f} | {tokenizer.decode(final_ranked_indices[-idx]):8s}')\n",
    "        print('\\n')\n",
    "        # add some variability here (so that we don't always take the best word)\n",
    "        best_word = tokenizer.decode(final_ranked_indices[-1])\n",
    "        prompt += best_word\n",
    "        print(f'Chose \\'{best_word.strip()}\\' to get: {prompt}')\n",
    "        print('-----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e17ba509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "0.076539 |  have   \n",
      "0.035736 |  use    \n",
      "0.035068 |  get    \n",
      "0.030946 |  take   \n",
      "0.029044 |  find   \n",
      "\n",
      "After Weighting: \n",
      "0.187514 |  have   \n",
      "0.145204 |  get    \n",
      "0.139645 |  take   \n",
      "0.139214 |  use    \n",
      "0.138316 |  find   \n",
      "\n",
      "[' find', ' use', ' take', ' get', ' have']\n",
      "tensor([0.1383, 0.1392, 0.1396, 0.1452, 0.1875], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# old stuff, using 'wte' matrix for the embeddings\n",
    "\n",
    "prompt = \"In the garden, I usually\"\n",
    "\n",
    "embeddings = model.transformer.wte.weight   # replace this matrix with the matrix we make\n",
    "\n",
    "# currently only supports cluster size of 1 (no looping/averaging)\n",
    "cluster = \"hate\"\n",
    "cluster_tokens = tokenizer(cluster, return_tensors=\"pt\")['input_ids'][0]\n",
    "cluster_embedding = embeddings[cluster_tokens]# if we had multiple members, would have to average here\n",
    "\n",
    "top_k_val = 5  # use top-p instead\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "#hidden_states = outputs.last_hidden_state\n",
    "#print(hidden_state)\n",
    "next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "# change values in next_token_scores[0] and then torch.sort to get indices\n",
    "#print(next_token_scores.shape)\n",
    "\n",
    "\n",
    "\n",
    "sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "# generate embedding for each index position\n",
    "# have 3 vectors, sorted_vals, indices, and embeddings. \n",
    "# turn embedding column into distance to cluster column\n",
    "# merge sorted_vals and embeddings columns (re-weight)\n",
    "# re-sort indices according to sorted_vals weights\n",
    "sorted_vals = sorted_vals[-top_k_val:]\n",
    "indices = indices[-top_k_val:]\n",
    "#print(indices)\n",
    "top_embeddings = embeddings[indices]\n",
    "#print(top_embeddings[0])\n",
    "#print(cluster_embedding)\n",
    "\n",
    "dist_score = [torch.linalg.norm(embed-cluster_embedding) for embed in top_embeddings]\n",
    "\n",
    "hyper_weight = .5\n",
    "\n",
    "checkpoint = sorted_vals.detach().clone()\n",
    "for i in range(len(sorted_vals)):\n",
    "    sorted_vals[i] += (1/dist_score[i])*hyper_weight\n",
    "\n",
    "sort_indices = torch.argsort(sorted_vals)\n",
    "final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "# best result is at the back of indices\n",
    "\n",
    "\n",
    "\n",
    "#print(sorted_vals)\n",
    "#print(sort_indices)\n",
    "print(\"Original: \")\n",
    "for idx in range(1, top_k_val+1):\n",
    "    #print(tokenizer.decode(indices[-idx]))\n",
    "    print(f'{checkpoint[-idx]:5f} | {tokenizer.decode(indices[-idx]):8s}')\n",
    "\n",
    "print()\n",
    "print(\"After Weighting: \")\n",
    "s_vals = sorted_vals[sort_indices]\n",
    "for idx in range(1, len(final_ranked_indices)+1):\n",
    "    #print(s_vals)\n",
    "    #print(idx)\n",
    "    print(f'{s_vals[-idx]:5f} | {tokenizer.decode(final_ranked_indices[-idx]):8s}')\n",
    "print()\n",
    "print([tokenizer.decode(word) for word in final_ranked_indices])\n",
    "print(s_vals)\n",
    "\n",
    "#next_token = next_token_scores.argmax().unsqueeze(0).unsqueeze(0)\n",
    "#print(tokenizer.decode(next_token[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
