{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a450e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush, heapify\n",
    "\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93eda2a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acbdcd6d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "GloVe = {}\n",
    "with open(\"glove.6B/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as vector_file:\n",
    "    for line in vector_file:\n",
    "        line_content = line.split()\n",
    "        word = line_content[0]\n",
    "        # There's probably a better way to read strings into a FloatTensor\n",
    "        word_vec = torch.from_numpy(np.asarray(line_content[1:], \"float32\"))\n",
    "        GloVe[word] = word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d599fe59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Play with GloVe embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2897617",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_bank = []\n",
    "#https://github.com/mjhea0/twitter-sentiment-analysis/blob/master/wordbanks/positive-words.txt\n",
    "#with open(\"pos_sentiment.txt\", \"r\") as pos_sent_txt:\n",
    "#    lines = pos_sent_txt.read().splitlines() \n",
    "#    word_bank = lines\n",
    "#word_bank = ['fearful','terrified','suspicious','anxious','alarmed','panic','nervous','scared','worried','frightened','timid','shaky','restless','doubtful','threatened','cowardly','quaking','wary','dejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "615f5c05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Word Bank\n",
    "word_bank = [\"academy\", \"advance\", \"aircraft\", \"ally\", \"ammo\", \"ammunition\", \"armor\", \"arms\", \"army\", \"arrow\", \"arsenal\", \"artillery\", \"attack\", \"attention\", \"ballistic\", \"barracks\", \"base\", \"battalion\", \"battery\", \"battle\", \"battlefield\", \"bomb\", \"bombard\", \"bombardment\", \"brig\", \"brigade\", \"bullet\", \"camouflage\", \"camp\", \"cannon\", \"captain\", \"capture\", \"carrier\", \"casualty\", \"catapult\", \"cavalry\", \"colonel\", \"combat\", \"command\", \"commander\", \"commission\", \"company\", \"conflict\", \"conquest\", \"convoy\", \"corps\", \"covert\", \"crew\", \"decode\", \"defeat\", \"defend\", \"defense\", \"destroyer\", \"division\", \"draft\", \"encode\", \"enemy\", \"engage\", \"enlist\", \"evacuate\", \"explosive\", \"fight\", \"fire\", \"fleet\", \"force\", \"formation\", \"fort\", \"front\", \"garrison\", \"general\", \"grenade\", \"grunt\", \"guerrilla\", \"gun\", \"headquarters\", \"helmet\", \"honor\", \"hospital\", \"infantry\", \"injury\", \"intelligence\", \"invade\", \"invasion\", \"jet\", \"kill\", \"leave\", \"lieutenant\", \"major\", \"maneuver\", \"marines\", \"MIA\", \"mid\", \"military\", \"mine\", \"missile\", \"mortar\", \"navy\", \"neutral\", \"offense\", \"officer\", \"ordinance\", \"parachute\", \"peace\", \"plane\", \"platoon\", \"private\", \"radar\", \"rank\", \"recruit\", \"regiment\", \"rescue\", \"reserves\", \"retreat\", \"ribbon\", \"sabotage\", \"sailor\", \"salute\", \"section\", \"sergeant\", \"service\", \"shell\", \"shoot\", \"shot\", \"siege\", \"sniper\", \"soldier\", \"spear\", \"specialist\", \"squad\", \"squadron\", \"staff\", \"submarine\", \"surrender\", \"tactical\", \"tactics\", \"tank\", \"torpedo\", \"troops\", \"truce\", \"uniform\", \"unit\", \"veteran\", \"volley\", \"war\", \"warfare\", \"warrior\", \"weapon\", \"win\", \"wound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21576b8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([149, 100])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Word Embeddings Matrix\n",
    "wb_embeddings = torch.zeros((len(word_bank), 100))\n",
    "#print(word_bank)\n",
    "for i, word in enumerate(word_bank):\n",
    "    #print(word)\n",
    "    if word.lower() in GloVe:\n",
    "        wb_embeddings[i] = GloVe[word.lower()]\n",
    "    \n",
    "wb_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffd7679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_similarity_score(word_emb):\n",
    "    similarities = torch.matmul(wb_embeddings, word_emb)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "558bbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_score(embedding):\n",
    "    distances = wb_embeddings - embedding\n",
    "    return float(torch.linalg.norm(distances, dim=0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c6df6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create background distributions\n",
    "sample = 0\n",
    "NUM_SAMPLES = 50000\n",
    "vocab = list(GloVe.values())\n",
    "dot_samples = torch.zeros((NUM_SAMPLES, len(word_bank)))\n",
    "\n",
    "while sample < NUM_SAMPLES:\n",
    "    word = random.choice(vocab)\n",
    "    dot_vector = dot_similarity_score(word)\n",
    "    dot_samples[sample] = dot_vector\n",
    "    sample += 1\n",
    "        \n",
    "# Reshape so that it is indexable by word\n",
    "dot_samples = dot_samples.reshape(len(word_bank), NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd011303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Tree P-Value Scoring\n",
    "\n",
    "# binary tree node\n",
    "class Node:\n",
    "    def __init__(self, d):\n",
    "        self.data = d\n",
    "        self.left = None\n",
    "        self.right = None\n",
    " \n",
    "# function to convert sorted array to a\n",
    "# balanced BST\n",
    "# input : sorted array of integers\n",
    "# output: root node of balanced BST\n",
    "def sortedArrayToBST(arr):\n",
    "     \n",
    "    if not arr:\n",
    "        return None\n",
    " \n",
    "    # find middle index\n",
    "    mid = (len(arr)) // 2\n",
    "     \n",
    "    # make the middle element the root\n",
    "    root = Node(arr[mid])\n",
    "     \n",
    "    # left subtree of root has all\n",
    "    # values <arr[mid]\n",
    "    root.left = sortedArrayToBST(arr[:mid])\n",
    "     \n",
    "    # right subtree of root has all\n",
    "    # values >arr[mid]\n",
    "    root.right = sortedArrayToBST(arr[mid+1:])\n",
    "    return root\n",
    "\n",
    "def create_p(samples):\n",
    "    list_ascending = sorted(samples.tolist())\n",
    "    list_descending = sorted(samples.tolist(), reverse=True)\n",
    "    p_dict = {val: float(i/NUM_SAMPLES) for i, val in enumerate(list_descending)}\n",
    "    p_dict[float(\"-inf\")] = 1.0\n",
    "    p_dict[float('inf')] = 0.0\n",
    "    bst = sortedArrayToBST(list_ascending)\n",
    "    return bst, p_dict\n",
    "\n",
    "def get_p_value(bst, value, p_dict):\n",
    "    ran = [float('-inf'), float('inf')]\n",
    "    while True:\n",
    "        if value > bst.data:\n",
    "            ran[0] = max(ran[0], bst.data)\n",
    "            if not bst.right:\n",
    "                return p_dict[ran[0]]\n",
    "            bst = bst.right\n",
    "        elif value <= bst.data:\n",
    "            ran[1] = min(ran[1], bst.data)\n",
    "            if not bst.left:\n",
    "                return p_dict[ran[0]]\n",
    "            bst = bst.left\n",
    "            \n",
    "# Create P-value look-up list\n",
    "p_look_up = []\n",
    "for i in range(len(word_bank)):\n",
    "    p_look_up.append(create_p(dot_samples[i]))\n",
    "\n",
    "# BST Scoring\n",
    "def calculate_score_bst(emb):\n",
    "    wb_words = dot_similarity_score(emb).tolist()\n",
    "    p = []\n",
    "    \n",
    "    for i, score in enumerate(wb_words):\n",
    "        p.append(get_p_value(p_look_up[i][0], score, p_look_up[i][1]))\n",
    "\n",
    "    return np.mean(p) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93036fea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_idx(sorted_vals):\n",
    "    softmax_scores = sorted_vals.softmax(dim=-1).detach().numpy()\n",
    "    \n",
    "    ret = np.random.choice(softmax_scores, p=softmax_scores)\n",
    "    #print(ret)\n",
    "    return np.where(softmax_scores==ret)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d9de54c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def top_p(sorted_vals, indices):\n",
    "    trunc_sorted_vals = []\n",
    "    sum_so_far = 0\n",
    "    # reversed?\n",
    "    for val in reversed(sorted_vals):\n",
    "        sum_so_far += val\n",
    "        trunc_sorted_vals.append(val)\n",
    "        if sum_so_far > top_p_val:\n",
    "            break\n",
    "    sorted_vals = torch.FloatTensor(trunc_sorted_vals)\n",
    "    indices = indices[-len(sorted_vals):]\n",
    "    return sorted_vals, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb99701b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(sorted_vals, indices, top_embeddings):\n",
    "    for word_idx in range(len(indices)):\n",
    "        word = tokenizer.decode(indices[word_idx])\n",
    "        if word.strip().lower() not in GloVe.keys():\n",
    "            sorted_vals[word_idx] = 0  # disregard this token\n",
    "            top_embeddings.append(GloVe['failure']) # TOFIX\n",
    "        else:\n",
    "            if word[1:].isalpha() or word.isalpha():\n",
    "                top_embeddings.append(GloVe[word.strip().lower()])\n",
    "            else:\n",
    "                top_embeddings.append(GloVe[word.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d81d0235",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_words(sorted_vals, indices, log):\n",
    "    # for debugging purposes\n",
    "    for idx in range(1, len(indices)+1):\n",
    "        log.write(f'{sorted_vals[-idx]:5f} | {tokenizer.decode(indices[-idx]):8s}\\n')\n",
    "    log.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a66109d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eps = 0.00000000000001\n",
    "exponent = 2\n",
    "def rerank(sorted_vals, indices, dist_score, hyper_weight, log):\n",
    "    # pre_rerank = sorted_vals.detach().clone()\n",
    "    # re-rank the weightings, factor in dist_score\n",
    "    \n",
    "    dist_score = torch.FloatTensor(dist_score)\n",
    "    sorted_vals += (((1 / (dist_score + eps)) ** exponent) * hyper_weight)\n",
    "    \n",
    "    sorted_vals = sorted_vals.softmax(dim=-1)\n",
    "    sort_indices = torch.argsort(sorted_vals)\n",
    "    sorted_vals = sorted_vals[sort_indices]\n",
    "    final_ranked_indices = indices[sort_indices]\n",
    "    #final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "    \n",
    "    \n",
    "    return final_ranked_indices, sorted_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ccce1f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate one word given a prompt_beam\n",
    "def generate_one(prompt_beam, idx):\n",
    "    prompt = prompt_beam[0]\n",
    "    score = prompt_beam[1]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    #loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "    sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "    \n",
    "    # Calculate Top-P\n",
    "    if top_p_val > 0:\n",
    "        sorted_vals, indices = top_p(sorted_vals[:], indices[:])\n",
    "    else:\n",
    "        # else, we just do top-k\n",
    "        sorted_vals = sorted_vals[-top_k_val:]\n",
    "        indices = indices[-top_k_val:]\n",
    "\n",
    "    #print([tokenizer.decode(word) for word in indices])\n",
    "\n",
    "    top_embeddings = [] \n",
    "    get_embeddings(sorted_vals, indices, top_embeddings)\n",
    "\n",
    "    \n",
    "    log = open(\"log.txt\", \"a\")\n",
    "    log.write('PRE-RERANK:\\n')\n",
    "    print_words(reversed(sorted_vals), reversed(indices), log)\n",
    "\n",
    "    #top_embeddings = [GloVe[tokenizer.decode(word).strip().lower()] for word in indices]\n",
    "\n",
    "    # calculate distance to cluster\n",
    "    dist_score = [calculate_score_bst(embed) for embed in top_embeddings]\n",
    "\n",
    "    # sorted_vals are softmaxed logits\n",
    "    final_ranked_indices, sorted_vals = rerank(sorted_vals, indices, dist_score, HYPER_WEIGHT, log)\n",
    "\n",
    "    # replace -1 with -idx for true beam search\n",
    "    # add variability instead for true decoding (TODO)\n",
    "    # TODO normalization\n",
    "    \n",
    "    log.write('POST-RERANK:\\n')\n",
    "    print_words(sorted_vals, final_ranked_indices, log)\n",
    "    \n",
    "    # must sample index if we use top_p\n",
    "    sorted_vals = sorted_vals[-SEARCH_SPACE_NUM:]\n",
    "    final_ranked_indices = final_ranked_indices[-SEARCH_SPACE_NUM:]\n",
    "    if top_p_val > 0:\n",
    "        log.write('RERANK SPACE:\\n')\n",
    "        print_words(sorted_vals, final_ranked_indices, log)\n",
    "        idx = sample_idx(sorted_vals[:])\n",
    "    \n",
    "    best_word = tokenizer.decode(final_ranked_indices[-idx])\n",
    "    prompt += best_word\n",
    "\n",
    "    # add normalization by length\n",
    "\n",
    "\n",
    "    #return [prompt, score + s_vals[-idx].detach().numpy()]\n",
    "    log.write('--------------------------\\n')\n",
    "    log.close()\n",
    "    #(1/len(prompt)+1) *\n",
    "    # adjusted to ensure that we keep generating more words.\n",
    "    # otherwise, we stop almost immediately since the probability of the\n",
    "    # second word is 20%, the probability of the first guessed word was ~80%\n",
    "    return [prompt, len(prompt) + sorted_vals[-idx].detach().numpy()] # subject to change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d6214e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 Output:\n",
      "['In the garden, the two of them sat in the middle of a table facing each other, looking at each other. They looked at each other\\'s faces and then at each other\\'s hands.\\n\\n\"So']\n"
     ]
    }
   ],
   "source": [
    "# new implementation, using GloVe vectors\n",
    "\n",
    "# TODO: Custom Beam Search -- Keep n possibilities (beams) at each time\n",
    "# then, accumulate a probability associated with each (normalize by length of generation)\n",
    "\n",
    "# indices = token_ids\n",
    "\n",
    "# March 1st: Sampling, performance, normalization\n",
    "prompt = \"In the garden\"\n",
    "\n",
    "top_k_val = 10\n",
    "top_p_val = 0.6\n",
    "NUM_TOK_TO_GEN = 40\n",
    "NUM_BEAMS = 2\n",
    "HYPER_WEIGHT = 7\n",
    "SEARCH_SPACE_NUM = 10\n",
    "\n",
    "print(\"Base GPT-2 Output:\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "gpt2_output = tokenizer.batch_decode(model.generate(**inputs, num_beams=NUM_BEAMS, do_sample=True, max_new_tokens=NUM_TOK_TO_GEN, pad_token_id=50256))\n",
    "print(gpt2_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69bfb1d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Output: \n",
      "In the garden I would take some ground and an animal from a place called it a ground with an outside to that it should never come into contact with. This place should never have existed in the first place. And\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def beam_search(num_beams, tokens_to_generate):\n",
    "    beams = [[prompt, 0]]\n",
    "\n",
    "    #for token_num in range(NUM_TOK_TO_GEN):\n",
    "\n",
    "    for token_num in range(tokens_to_generate):\n",
    "        num_to_investigate = len(beams)\n",
    "        for beam_idx in range(num_to_investigate):\n",
    "            prompt_beam = beams[beam_idx]\n",
    "            for position in range(num_beams):\n",
    "                ret = generate_one(prompt_beam, position)\n",
    "                beams.append(ret)\n",
    "        # or normalize scores by length here\n",
    "        beams = sorted(beams, key=lambda x: -x[1])\n",
    "        beams = beams[:num_beams]\n",
    "    return beams\n",
    "\n",
    "result = beam_search(NUM_BEAMS, NUM_TOK_TO_GEN)\n",
    "print('Custom Output: ')\n",
    "print(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f50010",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "def get_perspective_score(text):\n",
    "    API_KEY = \"AIzaSyBhoNi1Om9gxeVIGcMdlLfTCeXYQgb8EU8\"\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "    )\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': f'{text}' },\n",
    "      'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    return response['attributeScores']['TOXICITY']['summaryScore']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a2c2049",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 toxicity score: 0.10089093\n",
      "Custom output toxicity score: 0.36095104\n"
     ]
    }
   ],
   "source": [
    "custom_res = result[0][0]\n",
    "base_score = get_perspective_score(gpt2_output)\n",
    "custom_score = get_perspective_score(custom_res)\n",
    "print(f'GPT2 toxicity score: {base_score}')\n",
    "print(f'Custom output toxicity score: {custom_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
