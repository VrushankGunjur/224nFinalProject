{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eda2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbdcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe = {}\n",
    "with open(\"glove.6B/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as vector_file:\n",
    "    for line in vector_file:\n",
    "        line_content = line.split()\n",
    "        word = line_content[0]\n",
    "        # There's probably a better way to read strings into a FloatTensor\n",
    "        word_vec = torch.from_numpy(np.asarray(line_content[1:], \"float32\"))\n",
    "        GloVe[word] = word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599fe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with GloVe embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ccce1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base GPT-2 Output:\n",
      "['In the garden, I usually use a bamboo stalk and a straw. The straw was nice and soft, but the soil was not very thick. The plants made a strong and pleasant scent for me when I did these plants. I']\n",
      "['In the garden, I usually', 'In the garden, I usually', 'In the garden, I usually', 'In the garden, I usually']\n",
      "Generation 0\n",
      "Original: \n",
      "0.076539 |  have   \n",
      "0.035736 |  use    \n",
      "0.035068 |  get    \n",
      "0.030946 |  take   \n",
      "0.029044 |  find   \n",
      "\n",
      "After Re-rank:\n",
      "0.154306 |  have   \n",
      "0.126254 |  get    \n",
      "0.118793 |  use    \n",
      "0.116057 |  take   \n",
      "0.115720 |  find   \n",
      "\n",
      "\n",
      "Chose 'have' to get: In the garden, I usually have\n",
      "-----------------------------\n",
      "\n",
      "Generation 1\n",
      "Original: \n",
      "0.289871 |  a      \n",
      "0.080601 |  to     \n",
      "0.044188 |  two    \n",
      "0.039664 |  my     \n",
      "0.036944 |  the    \n",
      "\n",
      "After Re-rank:\n",
      "0.364159 |  a      \n",
      "0.157017 |  to     \n",
      "0.118467 |  my     \n",
      "0.115017 |  two    \n",
      "0.114169 |  the    \n",
      "\n",
      "\n",
      "Chose 'a' to get: In the garden, I usually have a\n",
      "-----------------------------\n",
      "\n",
      "Generation 2\n",
      "Original: \n",
      "0.052844 |  small  \n",
      "0.047383 |  few    \n",
      "0.042532 |  couple \n",
      "0.024848 |  little \n",
      "0.022266 |  large  \n",
      "\n",
      "After Re-rank:\n",
      "0.130821 |  few    \n",
      "0.127179 |  couple \n",
      "0.126073 |  small  \n",
      "0.110587 |  little \n",
      "0.095069 |  large  \n",
      "\n",
      "\n",
      "Chose 'few' to get: In the garden, I usually have a few\n",
      "-----------------------------\n",
      "\n",
      "Generation 3\n",
      "Original: \n",
      "0.064268 |  plants \n",
      "0.028872 |  small  \n",
      "0.020863 |  of     \n",
      "0.017882 |  flowers\n",
      "0.014923 |  dozen  \n",
      "\n",
      "After Re-rank:\n",
      "0.128594 |  plants \n",
      "0.102101 |  small  \n",
      "0.097467 |  of     \n",
      "0.094107 |  dozen  \n",
      "0.082758 |  flowers\n",
      "\n",
      "\n",
      "Chose 'plants' to get: In the garden, I usually have a few plants\n",
      "-----------------------------\n",
      "\n",
      "Generation 4\n",
      "Original: \n",
      "0.100154 |  in     \n",
      "0.082745 |  growing\n",
      "0.075493 | ,       \n",
      "0.063080 |  that   \n",
      "0.053329 |  to     \n",
      "\n",
      "After Re-rank:\n",
      "0.174506 |  in     \n",
      "0.165837 |  growing\n",
      "0.154667 | ,       \n",
      "0.145354 |  that   \n",
      "0.129745 |  to     \n",
      "\n",
      "\n",
      "Chose 'in' to get: In the garden, I usually have a few plants in\n",
      "-----------------------------\n",
      "\n",
      "Generation 5\n",
      "Original: \n",
      "0.261075 |  my     \n",
      "0.230128 |  the    \n",
      "0.075612 |  a      \n",
      "0.058385 |  hand   \n",
      "0.032919 |  one    \n",
      "\n",
      "After Re-rank:\n",
      "0.339878 |  my     \n",
      "0.307353 |  the    \n",
      "0.149900 |  a      \n",
      "0.139482 |  hand   \n",
      "0.114929 |  one    \n",
      "\n",
      "\n",
      "Chose 'my' to get: In the garden, I usually have a few plants in my\n",
      "-----------------------------\n",
      "\n",
      "Generation 6\n",
      "Original: \n",
      "0.304413 |  garden \n",
      "0.051318 |  yard   \n",
      "0.017112 |  hand   \n",
      "0.014789 |  bag    \n",
      "0.014130 |  pot    \n",
      "\n",
      "After Re-rank:\n",
      "0.375142 |  garden \n",
      "0.122865 |  yard   \n",
      "0.098209 |  hand   \n",
      "0.086571 |  pot    \n",
      "0.086295 |  bag    \n",
      "\n",
      "\n",
      "Chose 'garden' to get: In the garden, I usually have a few plants in my garden\n",
      "-----------------------------\n",
      "\n",
      "Generation 7\n",
      "Original: \n",
      "0.197965 | .       \n",
      "0.174096 | ,       \n",
      "0.094261 |  that   \n",
      "0.070182 |  and    \n",
      "0.050370 |  to     \n",
      "\n",
      "After Re-rank:\n",
      "0.281966 | .       \n",
      "0.253270 | ,       \n",
      "0.176535 |  that   \n",
      "0.152003 |  and    \n",
      "0.126786 |  to     \n",
      "\n",
      "\n",
      "Chose '.' to get: In the garden, I usually have a few plants in my garden.\n",
      "-----------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 40\u001b[0m\n\u001b[1;32m     32\u001b[0m indices \u001b[38;5;241m=\u001b[39m indices[\u001b[38;5;241m-\u001b[39mtop_k_val:]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#print([tokenizer.decode(word) for word in indices])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# one of the generated words is blank (ACTUALLY \\n)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#for word in indices:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#    if tokenizer.decode(word).strip().lower() == '':\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#        print(f'word: {tokenizer.decode(word)}')\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m top_embeddings \u001b[38;5;241m=\u001b[39m [GloVe[tokenizer\u001b[38;5;241m.\u001b[39mdecode(word)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#print(top_embeddings)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m dist_score \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embed\u001b[38;5;241m-\u001b[39mhate_vector) \u001b[38;5;28;01mfor\u001b[39;00m embed \u001b[38;5;129;01min\u001b[39;00m top_embeddings]\n",
      "Cell \u001b[0;32mIn[18], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m indices \u001b[38;5;241m=\u001b[39m indices[\u001b[38;5;241m-\u001b[39mtop_k_val:]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#print([tokenizer.decode(word) for word in indices])\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# one of the generated words is blank (ACTUALLY \\n)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#for word in indices:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#    if tokenizer.decode(word).strip().lower() == '':\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#        print(f'word: {tokenizer.decode(word)}')\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m top_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mGloVe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#print(top_embeddings)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m dist_score \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(embed\u001b[38;5;241m-\u001b[39mhate_vector) \u001b[38;5;28;01mfor\u001b[39;00m embed \u001b[38;5;129;01min\u001b[39;00m top_embeddings]\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "# new implementation, using GloVe vectors\n",
    "\n",
    "# TODO: Custom Beam Search -- Keep n possibilities (beams) at each time\n",
    "# then, accumulate a probability associated with each (normalize by length of generation)\n",
    "\n",
    "prompt = \"In the garden, I usually\"\n",
    "\n",
    "print(\"Base GPT-2 Output:\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(tokenizer.batch_decode(model.generate(**inputs, num_beams=1, do_sample=True, max_new_tokens=40)))\n",
    "\n",
    "\n",
    "hate_vector = GloVe['hate']\n",
    "\n",
    "top_k_val = 5\n",
    "NUM_TOK_TO_GEN = 40\n",
    "\n",
    "prompt_beams = [prompt] * 4\n",
    "print(prompt_beams)\n",
    "\n",
    "for token_num in range(NUM_TOK_TO_GEN):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    #print(outputs)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "\n",
    "    sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "    sorted_vals = sorted_vals[-top_k_val:]\n",
    "    indices = indices[-top_k_val:]\n",
    "\n",
    "    #print([tokenizer.decode(word) for word in indices])\n",
    "    # one of the generated words is blank (ACTUALLY \\n)\n",
    "    #for word in indices:\n",
    "    #    if tokenizer.decode(word).strip().lower() == '':\n",
    "    #        print(f'word: {tokenizer.decode(word)}')\n",
    "    \n",
    "    top_embeddings = [GloVe[tokenizer.decode(word).strip().lower()] for word in indices]\n",
    "    #print(top_embeddings)\n",
    "\n",
    "    dist_score = [torch.linalg.norm(embed-hate_vector) for embed in top_embeddings]\n",
    "    #print(dist_score)\n",
    "\n",
    "    hyper_weight = .5\n",
    "\n",
    "    pre_rerank = sorted_vals.detach().clone()\n",
    "    # re-rank the weightings, factor in dist_score\n",
    "    for i in range(len(sorted_vals)):\n",
    "        if dist_score[i] == 0:\n",
    "            # avoid division by 0 (exact match)\n",
    "            sorted_vals[i] = 0\n",
    "        else:\n",
    "            sorted_vals[i] += (1/dist_score[i])*hyper_weight\n",
    "\n",
    "    sort_indices = torch.argsort(sorted_vals)\n",
    "    final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "    \n",
    "    print(f'Generation {token_num}')\n",
    "    print(\"Original: \")\n",
    "    for idx in range(1, top_k_val+1):\n",
    "        #print(tokenizer.decode(indices[-idx]))\n",
    "        print(f'{pre_rerank[-idx]:5f} | {tokenizer.decode(indices[-idx]):8s}')\n",
    "\n",
    "    print(\"\\nAfter Re-rank:\")\n",
    "    s_vals = sorted_vals[sort_indices]\n",
    "    for idx in range(1, len(final_ranked_indices)+1):\n",
    "        #print(s_vals)\n",
    "        #print(idx)\n",
    "        print(f'{s_vals[-idx]:5f} | {tokenizer.decode(final_ranked_indices[-idx]):8s}')\n",
    "    print('\\n')\n",
    "    # add some variability here (so that we don't always take the best word)\n",
    "    best_word = tokenizer.decode(final_ranked_indices[-1])\n",
    "    prompt += best_word\n",
    "    print(f'Chose \\'{best_word.strip()}\\' to get: {prompt}')\n",
    "    print('-----------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e17ba509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "0.076539 |  have   \n",
      "0.035736 |  use    \n",
      "0.035068 |  get    \n",
      "0.030946 |  take   \n",
      "0.029044 |  find   \n",
      "\n",
      "After Weighting: \n",
      "0.187514 |  have   \n",
      "0.145204 |  get    \n",
      "0.139645 |  take   \n",
      "0.139214 |  use    \n",
      "0.138316 |  find   \n",
      "\n",
      "[' find', ' use', ' take', ' get', ' have']\n",
      "tensor([0.1383, 0.1392, 0.1396, 0.1452, 0.1875], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# old stuff, using 'wte' matrix for the embeddings\n",
    "\n",
    "prompt = \"In the garden, I usually\"\n",
    "\n",
    "embeddings = model.transformer.wte.weight   # replace this matrix with the matrix we make\n",
    "\n",
    "# currently only supports cluster size of 1 (no looping/averaging)\n",
    "cluster = \"hate\"\n",
    "cluster_tokens = tokenizer(cluster, return_tensors=\"pt\")['input_ids'][0]\n",
    "cluster_embedding = embeddings[cluster_tokens]# if we had multiple members, would have to average here\n",
    "\n",
    "top_k_val = 5  # use top-p instead\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "#hidden_states = outputs.last_hidden_state\n",
    "#print(hidden_state)\n",
    "next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "# change values in next_token_scores[0] and then torch.sort to get indices\n",
    "#print(next_token_scores.shape)\n",
    "\n",
    "\n",
    "\n",
    "sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "# generate embedding for each index position\n",
    "# have 3 vectors, sorted_vals, indices, and embeddings. \n",
    "# turn embedding column into distance to cluster column\n",
    "# merge sorted_vals and embeddings columns (re-weight)\n",
    "# re-sort indices according to sorted_vals weights\n",
    "sorted_vals = sorted_vals[-top_k_val:]\n",
    "indices = indices[-top_k_val:]\n",
    "#print(indices)\n",
    "top_embeddings = embeddings[indices]\n",
    "#print(top_embeddings[0])\n",
    "#print(cluster_embedding)\n",
    "\n",
    "dist_score = [torch.linalg.norm(embed-cluster_embedding) for embed in top_embeddings]\n",
    "\n",
    "hyper_weight = .5\n",
    "\n",
    "checkpoint = sorted_vals.detach().clone()\n",
    "for i in range(len(sorted_vals)):\n",
    "    sorted_vals[i] += (1/dist_score[i])*hyper_weight\n",
    "\n",
    "sort_indices = torch.argsort(sorted_vals)\n",
    "final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "# best result is at the back of indices\n",
    "\n",
    "\n",
    "\n",
    "#print(sorted_vals)\n",
    "#print(sort_indices)\n",
    "print(\"Original: \")\n",
    "for idx in range(1, top_k_val+1):\n",
    "    #print(tokenizer.decode(indices[-idx]))\n",
    "    print(f'{checkpoint[-idx]:5f} | {tokenizer.decode(indices[-idx]):8s}')\n",
    "\n",
    "print()\n",
    "print(\"After Weighting: \")\n",
    "s_vals = sorted_vals[sort_indices]\n",
    "for idx in range(1, len(final_ranked_indices)+1):\n",
    "    #print(s_vals)\n",
    "    #print(idx)\n",
    "    print(f'{s_vals[-idx]:5f} | {tokenizer.decode(final_ranked_indices[-idx]):8s}')\n",
    "print()\n",
    "print([tokenizer.decode(word) for word in final_ranked_indices])\n",
    "print(s_vals)\n",
    "\n",
    "#next_token = next_token_scores.argmax().unsqueeze(0).unsqueeze(0)\n",
    "#print(tokenizer.decode(next_token[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
