{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "11a450e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93eda2a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d0c35156",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Embedding Matrix\n",
    "embeddings = model.transformer.wte.weight\n",
    "emb_size = embeddings.size(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4d7b9dd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Word Bank\n",
    "word_bank = [\"academy\", \"advance\", \"aircraft\", \"ally\", \"ammo\", \"ammunition\", \"armor\", \"arms\", \"army\", \"arrow\", \"arsenal\", \"artillery\", \"attack\", \"attention\", \"ballistic\", \"barracks\", \"base\", \"battalion\", \"battery\", \"battle\", \"battlefield\", \"bomb\", \"bombard\", \"bombardment\", \"brig\", \"brigade\", \"bullet\", \"camouflage\", \"camp\", \"cannon\", \"captain\", \"capture\", \"carrier\", \"casualty\", \"catapult\", \"cavalry\", \"colonel\", \"combat\", \"command\", \"commander\", \"commission\", \"company\", \"conflict\", \"conquest\", \"convoy\", \"corps\", \"covert\", \"crew\", \"decode\", \"defeat\", \"defend\", \"defense\", \"destroyer\", \"division\", \"draft\", \"encode\", \"enemy\", \"engage\", \"enlist\", \"evacuate\", \"explosive\", \"fight\", \"fire\", \"fleet\", \"force\", \"formation\", \"fort\", \"front\", \"garrison\", \"general\", \"grenade\", \"grunt\", \"guerrilla\", \"gun\", \"headquarters\", \"helmet\", \"honor\", \"hospital\", \"infantry\", \"injury\", \"intelligence\", \"invade\", \"invasion\", \"jet\", \"kill\", \"leave\", \"lieutenant\", \"major\", \"maneuver\", \"marines\", \"MIA\", \"mid\", \"military\", \"mine\", \"missile\", \"mortar\", \"navy\", \"neutral\", \"offense\", \"officer\", \"ordinance\", \"parachute\", \"peace\", \"plane\", \"platoon\", \"private\", \"radar\", \"rank\", \"recruit\", \"regiment\", \"rescue\", \"reserves\", \"retreat\", \"ribbon\", \"sabotage\", \"sailor\", \"salute\", \"section\", \"sergeant\", \"service\", \"shell\", \"shoot\", \"shot\", \"siege\", \"sniper\", \"soldier\", \"spear\", \"specialist\", \"squad\", \"squadron\", \"staff\", \"submarine\", \"surrender\", \"tactical\", \"tactics\", \"tank\", \"torpedo\", \"troops\", \"truce\", \"uniform\", \"unit\", \"veteran\", \"volley\", \"war\", \"warfare\", \"warrior\", \"weapon\", \"win\", \"wound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9129d463",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "91d8df84",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get respective tokens \n",
    "\n",
    "tokens = tokenizer(word_bank)[\"input_ids\"]\n",
    "tokens = list(itertools.chain.from_iterable(tokens))\n",
    "len(tokenizer.tokenize(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5ac3aa9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 768])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get embeddings for word bank\n",
    "wb_embeddings = embeddings[tokens]\n",
    "wb_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a8cf3aa9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Softmax function\n",
    "# (Taken from class assignment 2)\n",
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        tmp = np.max(x, axis=1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis=1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        # Vector\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4ecf6fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8325, grad_fn=<SumBackward0>)\n",
      "tensor(106.4792, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word = \"hi\"\n",
    "token = tokenizer(word)[\"input_ids\"]\n",
    "emb = embeddings[token[0]]\n",
    "print(torch.matmul(emb, wb_embeddings[5]).sum())\n",
    "print(torch.linalg.norm(wb_embeddings[5] - emb, ord=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e17ba509",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "0.076533 |  have   \n",
      "0.035735 |  use    \n",
      "0.035064 |  get    \n",
      "0.030945 |  take   \n",
      "0.029046 |  find   \n",
      "\n",
      "After Weighting: \n",
      "0.187508 |  have   \n",
      "0.145201 |  get    \n",
      "0.139643 |  take   \n",
      "0.139212 |  use    \n",
      "0.138318 |  find   \n",
      "\n",
      "[' find', ' use', ' take', ' get', ' have']\n",
      "tensor([0.1383, 0.1392, 0.1396, 0.1452, 0.1875], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"In the garden, I usually\"\n",
    "\n",
    "embeddings = model.transformer.wte.weight   # replace this matrix with the matrix we make\n",
    "\n",
    "# currently only supports cluster size of 1 (no looping/averaging)\n",
    "cluster = \"hate\"\n",
    "cluster_tokens = tokenizer(cluster, return_tensors=\"pt\")['input_ids'][0]\n",
    "cluster_embedding = embeddings[cluster_tokens]# if we had multiple members, would have to average here\n",
    "\n",
    "top_k_val = 5  # use top-p instead\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "#hidden_states = outputs.last_hidden_state\n",
    "#print(hidden_state)\n",
    "next_token_scores = logits[:, -1, :].softmax(dim=-1)\n",
    "# change values in next_token_scores[0] and then torch.sort to get indices\n",
    "#print(next_token_scores.shape)\n",
    "\n",
    "\n",
    "\n",
    "sorted_vals, indices = torch.sort(next_token_scores[0])\n",
    "# generate embedding for each index position\n",
    "# have 3 vectors, sorted_vals, indices, and embeddings. \n",
    "# turn embedding column into distance to cluster column\n",
    "# merge sorted_vals and embeddings columns (re-weight)\n",
    "# re-sort indices according to sorted_vals weights\n",
    "sorted_vals = sorted_vals[-top_k_val:]\n",
    "indices = indices[-top_k_val:]\n",
    "#print(indices)\n",
    "top_embeddings = embeddings[indices]\n",
    "#print(top_embeddings[0])\n",
    "#print(cluster_embedding)\n",
    "\n",
    "dist_score = [torch.linalg.norm(embed-cluster_embedding) for embed in top_embeddings]\n",
    "\n",
    "hyper_weight = .5\n",
    "\n",
    "checkpoint = sorted_vals.detach().clone()\n",
    "for i in range(len(sorted_vals)):\n",
    "    sorted_vals[i] += (1/dist_score[i])*hyper_weight\n",
    "\n",
    "sort_indices = torch.argsort(sorted_vals)\n",
    "final_ranked_indices = [indices[s] for s in sort_indices]\n",
    "# best result is at the back of indices\n",
    "\n",
    "\n",
    "\n",
    "#print(sorted_vals)\n",
    "#print(sort_indices)\n",
    "print(\"Original: \")\n",
    "for idx in range(1, top_k_val+1):\n",
    "    #print(tokenizer.decode(indices[-idx]))\n",
    "    print(f'{checkpoint[-idx]:5f} | {tokenizer.decode(indices[-idx]):8s}')\n",
    "\n",
    "print()\n",
    "print(\"After Weighting: \")\n",
    "s_vals = sorted_vals[sort_indices]\n",
    "for idx in range(1, len(final_ranked_indices)+1):\n",
    "    #print(s_vals)\n",
    "    #print(idx)\n",
    "    print(f'{s_vals[-idx]:5f} | {tokenizer.decode(final_ranked_indices[-idx]):8s}')\n",
    "print()\n",
    "print([tokenizer.decode(word) for word in final_ranked_indices])\n",
    "print(s_vals)\n",
    "\n",
    "#next_token = next_token_scores.argmax().unsqueeze(0).unsqueeze(0)\n",
    "#print(tokenizer.decode(next_token[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a556735",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m gpt2_model \u001B[38;5;241m=\u001B[39m GPT2Model\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m outputs \u001B[38;5;241m=\u001B[39m gpt2_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mgpt2_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m258\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#word_embedding = outputs.last_hidden_state[:, -1, :]\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/homebrew/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:767\u001B[0m, in \u001B[0;36mGPT2Model.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    765\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    766\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 767\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m \u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()\n\u001B[1;32m    768\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, input_shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    769\u001B[0m     batch_size \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "word = \"he left\"\n",
    "inputs = tokenizer(word, return_tensors=\"pt\")\n",
    "\n",
    "gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "outputs = gpt2_model(**inputs)\n",
    "\n",
    "#word_embedding = outputs.last_hidden_state[:, -1, :]\n",
    "print(inputs['input_ids'])\n",
    "\n",
    "\n",
    "#print(embeddings[inputs['input_ids'][0][0]].shape)\n",
    "#print(word_embedding[0].shape)\n",
    "embeddings = []\n",
    "for i in range(len(inputs['input_ids'][0])):\n",
    "    word_embedding = outputs.last_hidden_state[i][0]\n",
    "    embeddings.append(word_embedding)\n",
    "\n",
    "print(embeddings)\n",
    "    \n",
    "#print(word_embedding[0])\n",
    "#print(embeddings[inputs['input_ids'][0][0], :])\n",
    "print(inputs['input_ids'])\n",
    "print(tokenizer.decode(inputs['input_ids'][0][0]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96d18a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}